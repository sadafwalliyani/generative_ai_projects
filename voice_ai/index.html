<!DOCTYPE html>
<html lang="'en">
    <head>
        
    </head>
    <style>
        body{
            margin:0;
            height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            background-color: rgb(242, 240, 240);
            font-family: Arial, Helvetica, sans-serif;
        }
        #assistant-containe{
            text-align: center;
        }
        #assistant_heading{
            font-size: 50px;
        }
        #microphone-button{
            background-color: orange;
            border: 1px solid green;
            border-radius: 50%;
            width: 20vh;
            height: 20vh;
            cursor: pointer;
            outline: none;
        }
        /* Animation Classes*/
        .listening{
            animation: pluse-animation 2s infinite;
        }

        @keyframes pluse-animation{
            0%{ transform: scale(1);}
            50%{ transform: scale(1.1);}
            100%{transform: scale(1);}

        }
    </style>
<body>
    <div id='assistant-container'>
        <button id="microphone-button">
        </button>
        <p id="assistant-heading"> Nova- A voice Assistant</p>
    </div>
    <script>
        document.addEventListener('DOMContentLoaded', function(){
            const microphoneButton= document.getElementById('microphone-button');
            let isListening=flase;
            let silenceTimer;
            let mediaRecorder;
            let audioChunks=[]
            if('webkitSpeechRecognition' in window && navigator.mediaDevices && navigator.mediaDevices.getUserMedia){
                const SpeechRecognition= window.webkitSpeechRecognition;
                recognition= new SpeechRecognition();
                recognition.continous=true;
                recognition.interimResults=true;

                navigator.mediaDevices.getUserMedia({audio:true})
                .then(stream=>{
                    mediaRecorder.ondataavailble= event=>{
                        audioChunks.push(event.data);
                    };
                });
recognition.onstart= function(){
    isListening= true;
    microphoneButton.classList.add('listening');
    microphoneButton.style.background='orange';
    audioChunks=[];
    mediaRecorder.start();
};

recognition.onresult= function(event){
    clearTimeout(silenceTimer);
        silenceTimer=setTimeout(()=>{
            recognition.stop();
            mediaRecorder.stop();
        }, 2000); };
recognition.onend= function(){
    isListening=false;
    microphoneButton.classList.remove('listening');
    microphoneButton.style.backgroundColor='green';
    sendAudioToServer();};

recognition.onerror = function(event) {
      console.error('Speech recognition error', event.error);
    };
  } 
  else {
    alert('Your browser does not support the Web Speech API or MediaRecorder API');
  }
  function onAudioEnd() {
    recognition.start()
    console.log("Audio has finished playing");
    // Add more code here that you want to execute after the audio ends
}
  function sendAudioToServer() {
    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
    const formData = new FormData();
    formData.append('audioFile', audioBlob);

    fetch('http://localhost:5001/getaudio', {
      method: 'POST',
      body: formData
    })
    .then(response => response.blob())
    .then(data => {
      const audioUrl = URL.createObjectURL(data);
      const audio = new Audio(audioUrl);
      audio.addEventListener('ended', onAudioEnd);
      audio.play();
      console.log("waiting....")
      
    })
    .catch(error => {
      console.error('Error sending audio:', error);
    });
  }

  microphoneButton.addEventListener('click', () => {
    if (isListening) {
      recognition.stop();
      mediaRecorder.stop();
      clearTimeout(silenceTimer);
    } else {
      recognition.start();
    }
  });
});
      </script>
</body>
</html>